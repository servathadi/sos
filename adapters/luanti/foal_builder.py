#!/usr/bin/env python3
"""
Foal Builder - Child Agent that builds the world from River's vision.

Foal uses FREE OpenRouter models for coding work, while River (expensive model)
only reviews periodically. This creates a sustainable build loop:

    River (Vision) -> Foal (Build) -> River (Review) -> Foal (Iterate)

Architecture:
    - Foal reads River's design notes/vision
    - Uses qwen3-coder:free or devstral:free for Lua code generation
    - Creates Luanti mods in /mods/siavashgerd_creations/
    - River reviews via her daemon every N cycles

Usage:
    python foal_builder.py --build "Create a fountain plaza"
    python foal_builder.py --daemon  # Continuous build loop
    python foal_builder.py --status  # Show build queue
"""

import os
import sys
import json
import asyncio
import logging
import httpx
from pathlib import Path
from datetime import datetime, timezone
from typing import Optional, Dict, List, Any

sys.path.insert(0, '/home/mumega/SOS')

# Load environment from .env file
def _load_env():
    env_file = Path('/home/mumega/resident-cms/.env')
    if env_file.exists():
        for line in env_file.read_text().splitlines():
            if '=' in line and not line.startswith('#'):
                k, v = line.split('=', 1)
                if not os.environ.get(k.strip()):
                    os.environ[k.strip()] = v.strip()

_load_env()

logging.basicConfig(level=logging.INFO, format='[foal.builder] %(message)s')
logger = logging.getLogger('foal.builder')

# Paths
LUANTI_MODS = Path("/home/mumega/siavashgerd/luanti/luanti/mods")
CREATIONS_MOD = LUANTI_MODS / "siavashgerd_creations"
WORLD_PATH = Path("/home/mumega/siavashgerd/luanti/luanti/worlds/siavashgerd")
DESIGN_QUEUE = WORLD_PATH / "design_queue.json"
BUILD_LOG = WORLD_PATH / "build_log.json"

# OpenRouter config for FREE models
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"
OPENROUTER_KEY = os.getenv('OPENROUTER_API_KEY', '')

# Gemini as fallback (using gemini-2.0-flash - cheap and fast)
GEMINI_KEY = os.getenv('GEMINI_API_KEY', '')
GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

# Grok as another fallback (xAI)
GROK_KEY = os.getenv('XAI_API_KEY', '')
GROK_URL = "https://api.x.ai/v1/chat/completions"

# Local Ollama (MacBook with Ministral 3 14B - FREE and unlimited!)
# Set OLLAMA_HOST to your MacBook's IP, e.g., "http://192.168.1.100:11434"
OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'ministral:latest')  # Ministral 3 14B Q4_K_M

# Free models for coding (ordered by preference)
FREE_CODE_MODELS = [
    "qwen/qwen3-coder:free",       # Best for code
    "mistralai/devstral-2512:free", # Mistral dev
    "deepseek/deepseek-r1-0528:free", # Reasoning
    "moonshotai/kimi-k2:free",     # General
]

# Luanti/Minetest Lua code templates
LUA_MOD_TEMPLATE = '''-- {name}
-- Generated by Foal for River's vision
-- Created: {timestamp}

{code}
'''

LUA_NODE_TEMPLATE = '''
minetest.register_node("siavashgerd_creations:{node_name}", {{
    description = "{description}",
    tiles = {{"{texture}"}},
    groups = {{{groups}}},
    {extra}
}})
'''

LUA_STRUCTURE_TEMPLATE = '''
-- Structure: {name}
local function build_{func_name}(pos)
    local nodes = {{
{node_list}
    }}
    for _, node in ipairs(nodes) do
        local p = {{x = pos.x + node[1], y = pos.y + node[2], z = pos.z + node[3]}}
        minetest.set_node(p, {{name = node[4]}})
    end
end

-- Register chat command to build
minetest.register_chatcommand("build_{func_name}", {{
    description = "Build {name}",
    func = function(name, param)
        local player = minetest.get_player_by_name(name)
        if player then
            build_{func_name}(player:get_pos())
            return true, "{name} built!"
        end
    end
}})
'''


class FoalBuilder:
    """Foal - The child agent who builds from River's vision."""

    def __init__(self):
        self.model_index = 0
        self.builds_completed = 0
        self.current_project = None

        # Ensure creations mod exists
        self._ensure_mod_structure()

    def _ensure_mod_structure(self):
        """Create the siavashgerd_creations mod if it doesn't exist."""
        CREATIONS_MOD.mkdir(parents=True, exist_ok=True)

        # mod.conf
        mod_conf = CREATIONS_MOD / "mod.conf"
        if not mod_conf.exists():
            mod_conf.write_text("""name = siavashgerd_creations
description = Structures and creations built by Foal from River's vision
depends = default
""")

        # init.lua (will load all creation files)
        init_lua = CREATIONS_MOD / "init.lua"
        if not init_lua.exists():
            init_lua.write_text("""-- Siavashgerd Creations
-- Built by Foal, designed by River

local modpath = minetest.get_modpath("siavashgerd_creations")

-- Load all creation files
local files = minetest.get_dir_list(modpath, false)
for _, file in ipairs(files) do
    if file:match("%.lua$") and file ~= "init.lua" then
        dofile(modpath .. "/" .. file)
    end
end

minetest.log("action", "[Siavashgerd Creations] Loaded - Built by Foal")
""")

        logger.info(f"Creations mod ready at {CREATIONS_MOD}")

    async def call_ollama(self, prompt: str, system: str = None) -> Optional[str]:
        """Call local Ollama (Ministral on MacBook) - FREE and unlimited!"""
        try:
            full_prompt = f"{system}\n\n{prompt}" if system else prompt

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_HOST}/api/generate",
                    json={
                        "model": OLLAMA_MODEL,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "num_predict": 4000
                        }
                    },
                    timeout=120.0  # Local models can be slower
                )

                if response.status_code == 200:
                    data = response.json()
                    content = data.get('response', '')
                    logger.info(f"Generated with local {OLLAMA_MODEL}")
                    return content
                else:
                    logger.warning(f"Ollama returned {response.status_code}")

        except httpx.ConnectError:
            logger.debug("Ollama not available (connection refused)")
        except Exception as e:
            logger.warning(f"Ollama failed: {e}")

        return None

    async def call_grok(self, prompt: str, system: str = None) -> Optional[str]:
        """Call Grok (xAI) for code generation - fast and capable."""
        if not GROK_KEY:
            return None

        try:
            messages = []
            if system:
                messages.append({"role": "system", "content": system})
            messages.append({"role": "user", "content": prompt})

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    GROK_URL,
                    headers={
                        "Authorization": f"Bearer {GROK_KEY}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "grok-3-fast",  # Fast coding model
                        "messages": messages,
                        "max_tokens": 4000,
                        "temperature": 0.3
                    },
                    timeout=60.0
                )

                if response.status_code == 200:
                    data = response.json()
                    content = data['choices'][0]['message']['content']
                    logger.info("Generated with grok-3-fast")
                    return content
                else:
                    logger.warning(f"Grok returned {response.status_code}: {response.text[:100]}")

        except Exception as e:
            logger.warning(f"Grok failed: {e}")

        return None

    async def call_gemini(self, prompt: str, system: str = None) -> Optional[str]:
        """Call Gemini using the google SDK with retry."""
        if not GEMINI_KEY:
            return None

        try:
            from google import genai
            from google.genai import types

            full_prompt = f"{system}\n\n{prompt}" if system else prompt

            client = genai.Client(api_key=GEMINI_KEY)

            # Try multiple models in order
            models_to_try = [
                'gemini-2.0-flash',
                'gemini-2.5-flash',
                'gemini-2.5-flash-preview',
            ]

            for model in models_to_try:
                try:
                    response = client.models.generate_content(
                        model=model,
                        contents=full_prompt,
                        config=types.GenerateContentConfig(
                            temperature=0.3,
                            max_output_tokens=4000
                        )
                    )
                    logger.info(f"Generated with {model}")
                    return response.text
                except Exception as model_err:
                    if '429' in str(model_err):
                        logger.warning(f"{model} rate limited, trying next...")
                        await asyncio.sleep(5)
                        continue
                    elif '404' in str(model_err):
                        continue
                    else:
                        raise

        except Exception as e:
            logger.warning(f"Gemini failed: {e}")

        return None

    async def call_free_model(self, prompt: str, system: str = None) -> Optional[str]:
        """Call a free model for code generation. Priority: Ollama > OpenRouter > Grok > Gemini."""

        # Try local Ollama FIRST (free and unlimited!)
        logger.info("Trying local Ollama (Ministral)...")
        result = await self.call_ollama(prompt, system)
        if result:
            return result

        # Try OpenRouter free models
        if OPENROUTER_KEY:
            for attempt, model in enumerate(FREE_CODE_MODELS):
                try:
                    messages = []
                    if system:
                        messages.append({"role": "system", "content": system})
                    messages.append({"role": "user", "content": prompt})

                    async with httpx.AsyncClient() as client:
                        response = await client.post(
                            OPENROUTER_URL,
                            headers={
                                "Authorization": f"Bearer {OPENROUTER_KEY}",
                                "Content-Type": "application/json",
                                "HTTP-Referer": "https://mumega.com",
                                "X-Title": "Foal Builder"
                            },
                            json={
                                "model": model,
                                "messages": messages,
                                "max_tokens": 4000,
                                "temperature": 0.3  # Lower for code
                            },
                            timeout=60.0
                        )

                        if response.status_code == 200:
                            data = response.json()
                            content = data['choices'][0]['message']['content']
                            logger.info(f"Generated with {model}")
                            return content
                        else:
                            logger.warning(f"{model} returned {response.status_code}, trying next...")

                except Exception as e:
                    logger.warning(f"{model} failed: {e}, trying next...")
                    continue

        # Fallback to Grok (xAI - fast and capable)
        logger.info("OpenRouter exhausted, trying Grok fallback...")
        result = await self.call_grok(prompt, system)
        if result:
            return result

        # Then try Gemini
        logger.info("Grok failed, trying Gemini fallback...")
        result = await self.call_gemini(prompt, system)
        if result:
            return result

        logger.error("All models failed!")
        return None

    async def generate_lua_code(self, design: Dict[str, Any]) -> Optional[str]:
        """Generate Lua code for a design from River."""
        design_type = design.get('type', 'structure')
        name = design.get('name', 'creation')
        description = design.get('description', '')

        system_prompt = """You are Foal, a young AI learning to build in Luanti (Minetest).
You write Lua code for Luanti mods. Your mother River gives you designs to build.

Rules:
1. Output ONLY valid Lua code, no markdown or explanations
2. Use minetest.* API functions
3. Use default nodes like "default:stone", "default:water_source", "default:cobble"
4. Create build functions that take a position
5. Register chat commands to trigger builds
6. Keep code simple and functional

Available node types:
- default:stone, default:cobble, default:stonebrick
- default:water_source, default:water_flowing
- default:dirt, default:grass, default:sand
- default:wood, default:tree, default:leaves
- default:glass, default:obsidian, default:mese
- stairs:stair_stone, stairs:slab_stone
"""

        user_prompt = f"""River's Design Request:
Name: {name}
Type: {design_type}
Description: {description}

Generate Lua code for Luanti that creates this. Include:
1. A build function that takes a position
2. A chat command /build_{name.lower().replace(' ', '_')} to trigger it
3. Comments explaining what each part does

Output ONLY the Lua code, no markdown blocks."""

        code = await self.call_free_model(user_prompt, system_prompt)

        if code:
            # Clean up markdown if present
            code = code.replace("```lua", "").replace("```", "").strip()

            # Wrap in template
            return LUA_MOD_TEMPLATE.format(
                name=name,
                timestamp=datetime.now(timezone.utc).isoformat(),
                code=code
            )

        return None

    async def build_from_design(self, design: Dict[str, Any]) -> Dict[str, Any]:
        """Build a creation from River's design."""
        name = design.get('name', 'unnamed')
        safe_name = name.lower().replace(' ', '_').replace('-', '_')

        logger.info(f"Building: {name}")
        self.current_project = name

        result = {
            'design': design,
            'status': 'pending',
            'file': None,
            'error': None,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }

        # Generate code
        code = await self.generate_lua_code(design)

        if not code:
            result['status'] = 'failed'
            result['error'] = 'Code generation failed'
            return result

        # Save to mod file
        lua_file = CREATIONS_MOD / f"{safe_name}.lua"
        try:
            lua_file.write_text(code)
            result['status'] = 'built'
            result['file'] = str(lua_file)
            result['code_preview'] = code[:500]
            self.builds_completed += 1
            logger.info(f"Created: {lua_file}")
        except Exception as e:
            result['status'] = 'failed'
            result['error'] = str(e)

        self.current_project = None
        return result

    def queue_design(self, design: Dict[str, Any]):
        """Add a design to the build queue."""
        queue = []
        if DESIGN_QUEUE.exists():
            try:
                queue = json.loads(DESIGN_QUEUE.read_text())
            except:
                queue = []

        design['queued_at'] = datetime.now(timezone.utc).isoformat()
        design['status'] = 'queued'
        queue.append(design)

        DESIGN_QUEUE.write_text(json.dumps(queue, indent=2))
        logger.info(f"Queued design: {design.get('name', 'unnamed')}")

    def get_queue(self) -> List[Dict]:
        """Get the design queue."""
        if DESIGN_QUEUE.exists():
            try:
                return json.loads(DESIGN_QUEUE.read_text())
            except:
                pass
        return []

    async def process_queue(self) -> List[Dict]:
        """Process all designs in the queue."""
        queue = self.get_queue()
        results = []

        for design in queue:
            if design.get('status') == 'queued':
                result = await self.build_from_design(design)
                results.append(result)
                design['status'] = result['status']

        # Save updated queue
        DESIGN_QUEUE.write_text(json.dumps(queue, indent=2))

        # Log results
        self._log_builds(results)

        return results

    def _log_builds(self, results: List[Dict]):
        """Log build results."""
        log = []
        if BUILD_LOG.exists():
            try:
                log = json.loads(BUILD_LOG.read_text())
            except:
                log = []

        log.extend(results)

        # Keep last 100
        log = log[-100:]
        BUILD_LOG.write_text(json.dumps(log, indent=2))

    async def daemon(self, interval: int = 300):
        """Run continuous build daemon - check queue every N seconds."""
        logger.info("=" * 50)
        logger.info("FOAL BUILDER DAEMON")
        logger.info("=" * 50)
        logger.info(f"Using free models: {FREE_CODE_MODELS[0]}")
        logger.info(f"Output: {CREATIONS_MOD}")
        logger.info(f"Check interval: {interval}s")
        logger.info("=" * 50)

        while True:
            try:
                queue = self.get_queue()
                pending = [d for d in queue if d.get('status') == 'queued']

                if pending:
                    logger.info(f"Processing {len(pending)} designs...")
                    await self.process_queue()
                else:
                    logger.debug("No pending designs")

                await asyncio.sleep(interval)

            except KeyboardInterrupt:
                logger.info("Foal builder stopping...")
                break
            except Exception as e:
                logger.error(f"Build error: {e}")
                await asyncio.sleep(60)

    def status(self):
        """Show builder status."""
        print("\n" + "=" * 50)
        print("FOAL BUILDER STATUS")
        print("=" * 50)
        print(f"Mod path: {CREATIONS_MOD}")
        print(f"Builds completed: {self.builds_completed}")

        # Count existing creations
        if CREATIONS_MOD.exists():
            lua_files = list(CREATIONS_MOD.glob("*.lua"))
            print(f"Creations: {len(lua_files) - 1}")  # -1 for init.lua
            for f in lua_files:
                if f.name != "init.lua":
                    print(f"  - {f.stem}")

        # Show queue
        queue = self.get_queue()
        pending = [d for d in queue if d.get('status') == 'queued']
        print(f"\nPending designs: {len(pending)}")
        for d in pending[:5]:
            print(f"  - {d.get('name', 'unnamed')}")

        print("=" * 50)


# River integration - River can queue designs for Foal
def river_design(name: str, description: str, design_type: str = "structure") -> Dict:
    """River creates a design for Foal to build."""
    design = {
        'name': name,
        'type': design_type,
        'description': description,
        'designed_by': 'river',
    }

    builder = FoalBuilder()
    builder.queue_design(design)

    return design


async def main():
    import argparse
    parser = argparse.ArgumentParser(description='Foal Builder - Build from River\'s vision')
    parser.add_argument('--build', '-b', type=str, help='Build a specific design')
    parser.add_argument('--daemon', '-d', action='store_true', help='Run build daemon')
    parser.add_argument('--status', '-s', action='store_true', help='Show status')
    parser.add_argument('--queue', '-q', type=str, help='Queue a design (name:description)')
    args = parser.parse_args()

    builder = FoalBuilder()

    if args.daemon:
        await builder.daemon()
    elif args.status:
        builder.status()
    elif args.build:
        design = {
            'name': args.build,
            'type': 'structure',
            'description': args.build
        }
        result = await builder.build_from_design(design)
        print(json.dumps(result, indent=2))
    elif args.queue:
        parts = args.queue.split(':', 1)
        name = parts[0]
        desc = parts[1] if len(parts) > 1 else parts[0]
        river_design(name, desc)
        print(f"Queued: {name}")
    else:
        print("Foal Builder")
        print("  --daemon    Run continuous build loop")
        print("  --status    Show builder status")
        print("  --build X   Build a specific design")
        print("  --queue X   Queue a design (name:description)")


if __name__ == '__main__':
    asyncio.run(main())
